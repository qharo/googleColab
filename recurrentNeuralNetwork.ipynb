{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrentNeuralNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNL6GHWxvCKdTRoEhyWAOLS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qharo/googleColab/blob/master/recurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpkd_7tKI0A",
        "outputId": "d455f7fd-91e1-494f-d6e3-5599bf4aa3cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#GET THE FILE\n",
        "path_to_file = keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#ENCODING\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "#DECODING\n",
        "def int_to_text(ints):\n",
        "    try:\n",
        "        ints = ints.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ' '.join(idx2char[ints])\n",
        "\n",
        "#MOLDING THE TRAINING EXAMPLES\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#MAKING INPUT/OUTPUT DATA\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#MAKE TRAINING BATCHES\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "        keras.layers.Dense(vocab_size),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "#CREATING A LOSS FUNCTION\n",
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "#COMPILE MODEL\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "#CHECKPOINTS\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, \n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "#TRAINING MODEL\n",
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])\n",
        "\n",
        "#LOADING MODEL\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM,RNN_UNITS, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "Epoch 1/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 2.5968\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.8751\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.6324\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.5030\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.4236\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3677\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3237\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2854\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2504\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2167\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1815\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1476\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1113\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0726\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0344\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9944\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9535\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9138\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8745\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8352\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7995\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7644\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7324\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7024\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6768\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.6504\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6286\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6090\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5896\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5728\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5556\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5437\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5301\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5199\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.5100\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.5000\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4914\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4836\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4773\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4707\n",
            "Type a starting string: romeo\n",
            "romeo'   d i s g r a c e ,   a n d   f r a i t h f u l   b r e a t h ,   p u b l i c   h i m !   h e   h a s   g i v e n   h i m   d e a t h   b y \n",
            " t h i s   s i m p l e   i n t e ,   w h i c h   s i g h s   t h e r e   w a s   s t a y . \n",
            " \n",
            " M E N E N I U S : \n",
            " W e   w o u l d   n o t \n",
            " d o   b e f o r e   s o m e   s t a t e   f r o m   h i m . \n",
            " \n",
            " P R O S P E R O : \n",
            " N o ,   f e a r   y o u   p a l l a n t   f o r   h i m . \n",
            " \n",
            " S O M E R L E : \n",
            " E v e n   s o ,   o r   t h r o u g h   i t . \n",
            " \n",
            " F i r s t   S e r v a n t : \n",
            " O ,   y e s ,   m y   l o r d ,   b u t   g e n t l e m a n   i n   t h e   m o r n i n g   c o m e s . \n",
            " \n",
            " K I N G   R I C H A R D   I I : \n",
            " C o u s i n ,   t h r o w   i t   i s :   W h y   d o   y o u   f r o w \n",
            " b r o t h e r ,   A   c a u s e ? \n",
            " \n",
            " S L Y : \n",
            " A m ,   o r   I ,   o r   S i g n i o r   L o r d   c o m e   y o u :   w h a t   b o r n   i n   R o m e ?   I Z A B E T H : \n",
            " W i n   t h o u s a n d   f l o o d s   a n d   f a l l   o f   w a r , \n",
            " A n d   f e w   I   f u r t h e r   w i l l   n o t   c a l l   m e   s c o M e ,   t o o   f a r m   b e f o r e   y o u   g o   w i t h   h i m . \n",
            " \n",
            " T Y B A L T : \n",
            " W h a t ,   a r t   t h o u ,   A l o x !   T h e n ,   f a r e w e l l ,   R i v e r \n",
            " W o u l d   n o t   o b e y   y o u   b o t h .   P e t c u s ! \n",
            " \n",
            " A N G E L O : \n",
            " W h a t   c a n   y o u   v o u c h e t   m e   d e m p e r ' d   b e f o r e , \n",
            " b u t   A P r i c i o n   w i l l   b e   m a r r i e d   y e a r f u l   p l a c e ; \n",
            " A n d   y e t   s h a l t   t e n d   t h e e   h e n c e ;   a n d   w e   m a y   w e a r   a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvhiY0HnMm9m",
        "outputId": "110f6dc4-927e-4229-b5e9-3f2fe799409e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "#ACTUALLY GENERATING TEXT\n",
        "def generate_text(model, start_string):\n",
        "    num_generate = 800\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temmperature = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions/temmperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string + ' '.join(text_generated))\n",
        "\n",
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: hello\n",
            "helloa d . \n",
            " \n",
            " T R A N I O : \n",
            " S o f t !   t h o u   k n o w ' s t   n o   l a w   t o   m e   a r e   r e a s o n ' d . \n",
            " \n",
            " B U C K I N G H A M : \n",
            " U p o n   t h e   o ' e r l o o d ,   b e c a u s e   I   w i l l   b e \n",
            " d a u g h t e r   i n   y o u r   h a n d : \n",
            " O ,   h e   i s   c o m e ,   l i k e   o n e :   w e   m u s t   d e p a r t ; \n",
            " B u t   f a r   i n   n e e d   w i l l   d r a w   y o u   n o   p o w e r ,   I   w o u l d \n",
            " H e   l o n g   h a v e   I   w a t c h   t h e   a i r   d o c k ? \n",
            " O   t h o u   b e l o v e d ?   o n l y   t h y   w i l l ! \n",
            " \n",
            " K I N G   R I C H A R D   I I I : \n",
            " A n d   f r o m   t h i s   i s l a n d ' s   i n s t r u c t i o n ,   h a n g   a   s u b j e c t , \n",
            " A n d   I   a m   g o i n g   w i t h   i n s t r u c t i o n   t o   h i s . \n",
            " A s   i f   y o u   s e e   a   k i n g ,   a n d   a j o t h e s ,   g r a c i o u s   V e r o n a ; \n",
            " W h i c h   w e   i n   V i n c e n t i o .   W h i c h   t r i c e   h a t h   m e   h i s   w o r t h y   m a g i s t r a t e s . \n",
            " \n",
            " P R I N C E : \n",
            " T h i s   s t a l e   b e   r a t h e r   a s   t h e   f a i r   d e g r e e s \n",
            " C a n   t e a c h   y o u ,   F o r   t h y   l a d y   B e   m o c k ' d ! \n",
            " \n",
            " H O R T E N S I O : \n",
            " F a i t h ,   a s   b i g   f a r e w e l l :   s w e e t   E s c a l u s , \n",
            " O f   p e n e t r a g e   b e   t h e   k i n g ' s   K i n g   L e w i s   a n d   M I I L A N D : \n",
            " A l l   l i f e   l i k e   m e !   a n d   b u t   t h i n k   h e r ? \n",
            " \n",
            " G R U M I O : \n",
            " G o o d   m o r r o w ,   n d   y o u   t o   a l l   o c c u s e   i t \n",
            " o u r   g e n e r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}