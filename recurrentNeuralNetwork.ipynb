{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrentNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnp7cDlYVEmxdbAoVvPCve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qharo/googleColab/blob/master/recurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpkd_7tKI0A",
        "outputId": "a95738dd-0b69-4b4a-b5d2-c759fbdadb9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#GET THE FILE\n",
        "path_to_file = keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#ENCODING\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "#DECODING\n",
        "def int_to_text(ints):\n",
        "    try:\n",
        "        ints = ints.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ' '.join(idx2char[ints])\n",
        "\n",
        "#MOLDING THE TRAINING EXAMPLES\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#MAKING INPUT/OUTPUT DATA\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#MAKE TRAINING BATCHES\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "        keras.layers.Dense(vocab_size),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "#CREATING A LOSS FUNCTION\n",
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "#COMPILE MODEL\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "#CHECKPOINTS\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, \n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "#TRAINING MODEL\n",
        "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])\n",
        "\n",
        "#LOADING MODEL\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM,RNN_UNITS, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Epoch 1/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 2.5955\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.9013\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.6460\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.5087\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.4273\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3700\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3241\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2858\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2491\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2143\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1793\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1432\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1062\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0675\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0277\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9867\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9454\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9047\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8654\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8261\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7900\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7551\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7235\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6938\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6676\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6438\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6203\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5996\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5830\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5656\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5511\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5371\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5248\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5145\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5052\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4945\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4864\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4791\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4724\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4675\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4621\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4554\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.4524\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4473\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4432\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4378\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4365\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4322\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4302\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4260\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4236\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4218\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4195\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4188\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4170\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4145\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4133\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4107\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4087\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4064\n",
            "Epoch 61/100\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4054\n",
            "Epoch 62/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4040\n",
            "Epoch 63/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4030\n",
            "Epoch 64/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4021\n",
            "Epoch 65/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4018\n",
            "Epoch 66/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4003\n",
            "Epoch 67/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4000\n",
            "Epoch 68/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3996\n",
            "Epoch 69/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3999\n",
            "Epoch 70/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3956\n",
            "Epoch 71/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3941\n",
            "Epoch 72/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3955\n",
            "Epoch 73/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3933\n",
            "Epoch 74/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3937\n",
            "Epoch 75/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3915\n",
            "Epoch 76/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3897\n",
            "Epoch 77/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3902\n",
            "Epoch 78/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3911\n",
            "Epoch 79/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3913\n",
            "Epoch 80/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3910\n",
            "Epoch 81/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3910\n",
            "Epoch 82/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3885\n",
            "Epoch 83/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3876\n",
            "Epoch 84/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3879\n",
            "Epoch 85/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3879\n",
            "Epoch 86/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3869\n",
            "Epoch 87/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3873\n",
            "Epoch 88/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3874\n",
            "Epoch 89/100\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.3867\n",
            "Epoch 90/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3880\n",
            "Epoch 91/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3874\n",
            "Epoch 92/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3858\n",
            "Epoch 93/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3826\n",
            "Epoch 94/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3837\n",
            "Epoch 95/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3845\n",
            "Epoch 96/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3831\n",
            "Epoch 97/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3830\n",
            "Epoch 98/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3806\n",
            "Epoch 99/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3836\n",
            "Epoch 100/100\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.3835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvhiY0HnMm9m",
        "outputId": "8ddf0760-28d0-426c-fcfa-852beb95034e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "#ACTUALLY GENERATING TEXT\n",
        "def generate_text(model, start_string):\n",
        "    num_generate = 800\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temmperature = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions/temmperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string + ' '.join(text_generated))\n",
        "\n",
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: good eve\n",
            "good even t . \n",
            " \n",
            " G L O U C E S T E R : \n",
            " S a y ,   d o   n o t   s w e a r ,   m y   l o r d   o f   B u c k i n g h a m . \n",
            " \n",
            " C A T E S B E R : \n",
            " O   m y   m i s e r y ,   P e w i s   a n d   t i l l   t h e i r   p o w e r \n",
            " O f   g u i l t y   o f   t h e   w e l k i n g   s t a d d s ,   t y b a l t   f r o m   m e ! \n",
            " W h o   t o l d   m e ,   g e n t l e   N o r f o l k ,   t h u s   e x c h a n g e , \n",
            " O f   t h e   m o s t   s e e d s   w i t h   h e r ,   l e t   h e r   b e   w h a t   l o v e   i n   d e a t h ! \n",
            " \n",
            " C l o w n : \n",
            " A l a c k ,   p o o r   M a r g a r e t ,   y o u   b u y   d i e   t h e r e ,   m y   f r i e n d ? \n",
            " I s   n e i t h e r   h e a d ,   b e i n g   n e ' e r   s i t h \n",
            " T h a t   w a n t   n o   e a r s   a b r o a d - - \n",
            " T o   p l a g u e   t h e e   f o r   t h e   f i e l d   t o   g o ' n . \n",
            " \n",
            " K I N G   H E N R Y   V I : \n",
            " S i r   W i l l i a m   B r a n d o n ,   a n d   y o u   b o t h   s h e   s i n g s \n",
            " T h a t   t h o u   b e   m a s t e r ;   t h e r e f o r e ,   n a y ,   a n d   m y   p r o c e e d i n g s \n",
            " H a s t   t h o u   b e t t e r   h o w   t o   c o m e   t o   m e r c y ; \n",
            " I n   a l l   t h e   a i r   o f   t h i s   t i m e , \n",
            " F o r   o n e   p e r h a p s   w e   s i t   u n t o   h i s   c u r r a n :   a   b a w d ;   h e   t h i n k s   n o t \n",
            " c o m e   s o r r o w   o n   t h e   m a r k e t   p l a g u e   t h a t   e ' e r   I   g u e s s . \n",
            " \n",
            " K I N G   R I C H A R D   I I : \n",
            " R a g e   o n l y ,   t h a t   v a l i a n t   c h i l d \n",
            " W h e r e   s h e d   h i m s e l f   a   b a y f f i e r e d \n",
            " T\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}